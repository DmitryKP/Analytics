{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "from tqdm import tqdm_notebook\n",
    "from datetime import datetime, date, time\n",
    "import numpy as np\n",
    "pd.set_option('display.max_columns', 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_excel(r'C:\\Users\\korpachev\\Comments_Semantic_Analysis\\Data_Comments.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ufal.udpipe import Model, Pipeline\n",
    "model = Model.load(\"russian-syntagrus-ud-2.0-170801.udpipe\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(model, 'generic_tokenizer', '', '', '')\n",
    "example = \"Очень комфортно взаимодействовать с менеджером\"\n",
    "parsed = pipeline.process(example)\n",
    "parsed.split('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Анализ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('treaten/Менеджер- Менеджер К.pickle', 'rb') as f:\n",
    "    lemmas_list, lemmas_clear_list = pickle.load(f)\n",
    "def elementFreq(myList):\n",
    "    #myList is the list of lists\n",
    "    from collections import Counter\n",
    "    tmp = []\n",
    "    for i in myList: tmp += i        \n",
    "    return(Counter(tmp))\n",
    "    \n",
    "lemmas_clear_list = [e for i in lemmas_clear_list for e in i]\n",
    "lemmas_clear_list = pd.DataFrame.from_dict(elementFreq(lemmas_clear_list).most_common())\n",
    "l_noun = []\n",
    "for j in list(lemmas_clear_list[0]):\n",
    "    parsed = pipeline.process(j)\n",
    "    v1 = 'NOT'\n",
    "    for e in parsed.split('\\n'):\n",
    "        if e.split('\\t')[0] == '1' and  e.split('\\t')[3] == 'NOUN':\n",
    "            v1 = 'NOUN'\n",
    "    l_noun.append(v1)\n",
    "lemmas_clear_list['noun'] = l_noun\n",
    "lemmas_clear_list = lemmas_clear_list[lemmas_clear_list['noun'] == 'NOUN']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmas_clear_list.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def elementFreq(myList):\n",
    "    #myList is the list of lists\n",
    "    from collections import Counter\n",
    "    tmp = []\n",
    "    for i in myList: tmp += i        \n",
    "    return(Counter(tmp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_list_main = []\n",
    "\n",
    "qlist = [['Менеджер', ' Менеджер К'],['Личный кабинет', 'Личный кабинет К'],['Приложение', 'Приложение К'],['Офис', 'Офис К'],['Маркетинг', 'Маркетинг К'],['NPS','Что нам необходимо сделать, чтобы в следующий раз вы поставили оценку 10? Пожалуйста, оставьте комментарий']][0:-1]\n",
    "for category in qlist[0:-1][0:1]:\n",
    "    print(' '.join(u))\n",
    "    u = category    \n",
    "    q1 = data[data[u[0]] != 'Не было взаимодействия'][u]\n",
    "    q1 = q1[q1[u[1]] != '  ']\n",
    "    \n",
    "    with open('treaten/{}.pickle'.format('-'.join(u)), 'rb'.format()) as f:\n",
    "        lemmas_list, lemmas_clear_list = pickle.load(f)\n",
    "\n",
    "    lemmas_clear_list = [e for i in lemmas_clear_list for e in i]\n",
    "    lemmas_clear_list = pd.DataFrame.from_dict(elementFreq(lemmas_clear_list).most_common())\n",
    "    l_noun = []\n",
    "    for j in list(lemmas_clear_list[0]):\n",
    "        parsed = pipeline.process(j)\n",
    "        v1 = 'NOT'\n",
    "        for e in parsed.split('\\n'):\n",
    "            if e.split('\\t')[0] == '1' and  e.split('\\t')[3] == 'NOUN':\n",
    "                v1 = 'NOUN'\n",
    "        l_noun.append(v1)\n",
    "    lemmas_clear_list['noun'] = l_noun\n",
    "    lemmas_clear_list = lemmas_clear_list[lemmas_clear_list['noun'] == 'NOUN']\n",
    "    display(lemmas_clear_list.head(20))\n",
    "\n",
    "    df_list = []\n",
    "    \n",
    "    for word_m in tqdm_notebook(lemmas_clear_list.head(15)[0]):\n",
    "        print(word_m)\n",
    "        word_main = word_m.lower()\n",
    "        \n",
    "        ep_fin = []\n",
    "        e_fin = []\n",
    "        sent_fin = []\n",
    "        \n",
    "        for row in list(q1[u[1]]):\n",
    "            parsed = pipeline.process(str(row))\n",
    "            ep_row = []\n",
    "            e_row = []\n",
    "            sent_row = []\n",
    "            for p in parsed.split(\"sent_id\")[1:]:\n",
    "\n",
    "                full_sent = ' '.join([e for m in p.split(\"\\n\")[2:-2] for i,e in enumerate(m.split(\"\\t\")) if i in [1]])\n",
    "                entities = [[e.split('\\t')[0], e.split('\\t')[2]] for e in p.split(\"\\n\")[2:-2] if e.split('\\t')[3] == 'NOUN' and e.split('\\t')[2].lower() == word_main]\n",
    "\n",
    "                if entities != []: # Если внутри предложения в строчке нет entity - [] не добавляется\n",
    "                    for e in entities:\n",
    "                        entity_only= e[1].lower()\n",
    "                        entity_phrase = e[1]\n",
    "                        for m in p.split(\"\\n\")[2:-2]:\n",
    "                            if m.split('\\t')[6] == e[0]:\n",
    "                                if m.split('\\t')[3] == 'ADJ':\n",
    "                                       entity_phrase = (entity_phrase + ' ' + m.split('\\t')[2]).lower()\n",
    "                    ep_row.append(entity_phrase)\n",
    "                    e_row.append(entity_only)\n",
    "                    sent_row.append(full_sent)\n",
    "            ep_fin.append(ep_row)\n",
    "            e_fin.append(e_row)\n",
    "            sent_fin.append(sent_row)\n",
    "\n",
    "        ep_fin = [e for i in ep_fin for e in i] \n",
    "        e_fin = [e for i in e_fin for e in i] \n",
    "        sent_fin = [e for i in sent_fin for e in i] \n",
    "\n",
    "        df_raw = pd.DataFrame({'entity_full': ep_fin, 'sentence': sent_fin })\n",
    "        df = df_raw.groupby('entity_full')['sentence'].apply(list).to_frame().reset_index()\n",
    "        df['count'] = df['sentence'].apply(lambda x: len(x))\n",
    "        df.sort_values('count',ascending=False, inplace=True)\n",
    "        df['sentence'] = df['sentence'].apply(lambda x: ' | '.join(x))\n",
    "        df.rename(columns={'entity_full': 'Сущность + прилагательное', 'sentence': 'Фразы', 'count': 'Частота'}, inplace=True)\n",
    "\n",
    "        df_0 = df[df['Сущность + прилагательное'] == word_main]\n",
    "        df_0['agg'] = 1000000\n",
    "        df_0['Основная сущность + прилагательное'] = df_0['Сущность + прилагательное']\n",
    "\n",
    "        df_1 = df[df['Сущность + прилагательное'] != word_main]\n",
    "        import heapq\n",
    "        ler = list(df_1['Сущность + прилагательное'])\n",
    "        l1 = [e.replace('менеджер','').strip() for e in ler if e.replace(word_main,'').strip() != '']\n",
    "\n",
    "        stf= []\n",
    "        for m in l1:\n",
    "            stemp = []\n",
    "            for e in data_syms:\n",
    "                for i, j in enumerate(e):\n",
    "                    if j == m:\n",
    "                        stemp.append(e)\n",
    "            stemp = list(set([k for z in stemp for k in z]))\n",
    "            stf.append(stemp)\n",
    "\n",
    "        t2=[]\n",
    "        t2_indexes = []\n",
    "        for e in stf:\n",
    "            t1= []\n",
    "            for m in stf:\n",
    "                t1.append(len(list(set(e) & set(m))))\n",
    "            t2.append(t1)\n",
    "            max2 = heapq.nlargest(2, t1)[-1] \n",
    "\n",
    "            if max2 == 0:\n",
    "                t2_indexes.append(1000000)\n",
    "            else:\n",
    "                t2_indexes.append(t1.index(max2))\n",
    "\n",
    "        df_1['agg'] = t2_indexes\n",
    "        df_2 = df_1[df_1['agg'] == 1000000]\n",
    "        df_2['Основная сущность + прилагательное'] = df_2['Сущность + прилагательное']\n",
    "        df_1 = df_1[df_1['agg'] != 1000000]\n",
    "        df_1 = df_1.groupby('agg').agg({'Сущность + прилагательное' : list,'Фразы' : list, 'Частота': sum }).sort_values('Частота',ascending=False)\n",
    "        df_1['agg'] = 1000000\n",
    "        df_1['Основная сущность + прилагательное'] = df_1['Сущность + прилагательное'].apply(lambda x: x[0])\n",
    "        df_1['Сущность + прилагательное'] = df_1['Сущность + прилагательное'].apply(lambda x: ' | '.join(x))\n",
    "        df_1['Фразы'] = df_1['Фразы'].apply(lambda x: ' | '.join(x))\n",
    "        df = pd.concat([df_0,df_1,df_2],axis=0)[['Основная сущность + прилагательное','Сущность + прилагательное','Фразы','Частота']]             \n",
    "        df['Фильтр по медиане'] = np.where((df['Частота'] >= df['Частота'].median()), 'Выше медианы', 'Ниже медианы')\n",
    "        df.reset_index(drop=True, inplace=True)\n",
    "        #df.to_excel('treaten_sem_anal/{}.xlsx'.format(word_main),index=False)\n",
    "        df[''] = ''\n",
    "        print(df.shape)\n",
    "        df_list.append(df)\n",
    "\n",
    "        ep_fin = []\n",
    "        e_fin = []\n",
    "        sent_fin = []\n",
    "        for row in list(q1[u[1]]):\n",
    "            parsed = pipeline.process(str(row))\n",
    "            ep_row = []\n",
    "            e_row = []\n",
    "            sent_row = []\n",
    "            for p in parsed.split(\"sent_id\")[1:]:\n",
    "                full_sent = ' '.join([e for m in p.split(\"\\n\")[2:-2] for i,e in enumerate(m.split(\"\\t\")) if i in [1]])\n",
    "                entities = [[e.split('\\t')[6], e.split('\\t')[2]] for e in p.split(\"\\n\")[2:-2] if e.split('\\t')[3] == 'NOUN' and e.split('\\t')[2].lower() == word_main]\n",
    "\n",
    "                if entities != []: # Если внутри предложения в строчке нет entity - [] не добавляется\n",
    "                    for e in entities:\n",
    "                        entity_only= e[1].lower()\n",
    "                        entity_phrase = e[1]\n",
    "                        verb_form = ''\n",
    "                        for m in p.split(\"\\n\")[2:-2]:\n",
    "                            if m.split('\\t')[0] == e[0]:\n",
    "                                if m.split('\\t')[3] == 'VERB':\n",
    "                                    verb_id = m.split('\\t')[0]\n",
    "                                    verb_form = m.split('\\t')[2]\n",
    "                                    for j in p.split(\"\\n\")[2:-2]:\n",
    "                                        if j.split('\\t')[6] == verb_id:\n",
    "                                            if j.split('\\t')[7] == 'advmod':\n",
    "                                                if j.split('\\t')[3] == 'PART':\n",
    "                                                    verb_form = j.split('\\t')[2] + \" \" + verb_form\n",
    "                                                else:\n",
    "                                                    verb_form = verb_form + \" \" + j.split('\\t')[2]\n",
    "                                            else:\n",
    "                                                if j.split('\\t')[3] == 'NOUN' and j.split('\\t')[2].lower() != word_main:\n",
    "\n",
    "                                                    verb_form = verb_form + \" \" + j.split('\\t')[2]\n",
    "\n",
    "                        entity_phrase = (entity_phrase + \" \" + verb_form).lower()\n",
    "                        ep_row.append(entity_phrase)\n",
    "                        e_row.append(entity_only)\n",
    "                        sent_row.append(full_sent)\n",
    "            ep_fin.append(ep_row)\n",
    "            e_fin.append(e_row)\n",
    "            sent_fin.append(sent_row)\n",
    "            \n",
    "        ep_fin = [e for i in ep_fin for e in i] \n",
    "        e_fin = [e for i in e_fin for e in i] \n",
    "        sent_fin = [e for i in sent_fin for e in i] \n",
    "\n",
    "        df_raw = pd.DataFrame({'entity_full': ep_fin, 'sentence': sent_fin })\n",
    "        df = df_raw.groupby('entity_full')['sentence'].apply(list).to_frame().reset_index()\n",
    "        df['count'] = df['sentence'].apply(lambda x: len(x))\n",
    "        df.sort_values('count',ascending=False, inplace=True)\n",
    "        df['sentence'] = df['sentence'].apply(lambda x: ' | '.join(x))\n",
    "        df.rename(columns={'entity_full': 'Сущность + глагол', 'sentence': 'Фразы', 'count': 'Частота'}, inplace=True)\n",
    "        #    df = df[df['Частота'] > df['Частота'].median()]\n",
    "        df['Фильтр по медиане'] = np.where((df['Частота'] >= df['Частота'].median()), 'Выше медианы', 'Ниже медианы')\n",
    "        df.reset_index(drop=True, inplace=True)\n",
    "        df[''] = ''\n",
    "        print(df.shape)\n",
    "        df_list.append(df)\n",
    "    print('Добавляют все сущности по вопросам в категории')\n",
    "    df_list_main.append(pd.concat(df_list,axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
